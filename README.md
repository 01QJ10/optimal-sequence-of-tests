# Optimal Sequence of Tests

This repository implements methods for selecting an optimal subset of tests (questions, measurements, etc.) to minimize posterior variance, as introduced by the original authors citessrn5118887. Two approaches are provided:

1. **Fast (prefix) method** based on Definition 1 and Corollary 6 from the paper.
2. **Slow (brute‑force) method** using Lemma 9 from the paper.

---

## Repository Structure

```
- code
    - demo.ipynb
    - preprocess.ipynb
    - testSamples.py        # Utilities for testing diagonal‑dominance
    - module.py             # Core functions: is_diagonally_dominant, optimal_test_selection
    - main.py               # CLI to load data, select sequence, write output
- tests
    - diagonalMatrices      # Invertible principal submatrices for positive tests
    - nonDiagonalMatrices   # Matrices failing the diagonal‑dominance check
- output                    # Results generated by `main.py` go here
- data
    - normalTests.csv       # Input CSV for `main.py` (questions + numeric data)
    - log_normality_results.csv
    - normality_results.csv
    - normality_test.xlsx
```

---

## Instructions

1. **Prepare your input CSV** by running `preprocess.ipynb`.  The cleaned `normalTests.csv` must have:

   | Test Name    | Q1 Description                                                | Q2 Description | … |
   | ------------ | ------------------------------------------------------------- | -------------- | - |
   | **Header:**  | name of test (e.g. “CSQ‑VSF Globality”)                       | …              |   |
   | **Row 1:**   | textual question or prompt for each test                      | …              |   |
   | **Rows 2+**: | numeric scores for each participant (one row per participant) | …              |   |

2. **Run**

   ```bash
   python code/main.py
   ```

   * Enter **sequence length** (number of tests to select)
   * Enter **path** to `normalTests.csv`
   * Choose **fast** (prefix) or **slow** (Lemma 9) method
   * Enter **output** filename; result saved under `output/` as `.txt`

---

## Theoretical Background

### Definition 1 (Precision‑Prefix Dominance)

> A covariance matrix $\Sigma$ is said to be **diagonally‑dominant** if, after ordering indices so that
>
> $$
>   υ_{l_0}^{-2} \ge υ_{l_1}^{-2} \ge \cdots \ge υ_{l_{K-1}}^{-2},
> $$
>
> the sum of all entries of the inverse of every principal $k$-submatrix
> $\Sigma_{\{l_0,\dots,l_{k-1}\}}^{-1}$ is at least as large as that of **any** other $k$-subset.
> (Precision is defined as $υ_i^{-2}=1/v_i^2$, the reciprocal of variance.)

### Corollary 6 (Optimal Prefix)

> If $\Sigma$ satisfies Definition 1, then for **every** $k\le K$, the unique optimal $k$-subset
> that maximizes
> $J(S)=1^\top\Sigma_S^{-1}1$
> is exactly the **first $k$ indices** $\{l_0,\dots,l_{k-1}\}$.  Thus one need only sort by precision and take a prefix.

### Lemma 9 (Brute‑Force Optimality)

> In the general case (no prefix‑dominance assumed), the optimal $k$-subset $S^*$ is the one that
> maximizes
> $J(S)=1^\top\Sigma_S^{-1}1$
> among all $\binom{K}{k}$ choices.  Computing this requires enumerating each $k$-combination and inverting its submatrix.

---

## Example: 4×4 Covariance Matrix

Let

```python
Σ = np.array([
    [4.0, 1.2, 0.5, 0.3],
    [1.2, 2.0, 0.4, 0.6],
    [0.5, 0.4, 3.0, 0.2],
    [0.3, 0.6, 0.2, 5.0]
])
```

1. **Check Definition 1** for global prefix‑dominance:

   * Variances = $[4,2,3,5]$ → Precisions = $[0.25,0.5,0.333,0.2]$
   * Sort descending: indices $(1,2,0,3)$
   * For $k=1$: best is $\{1\}$ since $0.5\ge0.333,0.25,0.2$
   * For $k=2$: compare $\{1,2\}$ vs all other pairs; confirm its inverse‑sum is largest
   * … holds through $k=4$.  So $Σ$ **is** diagonally‑dominant.

2. **Fast method** for $k=3$: take prefix $(1,2,0)$.

3. **Lemma 9** brute‑force for $k=3$:

   ```python
   best_score, best_set = -inf, None
   for S in combinations(range(4),3):
       inv = np.linalg.inv(Σ[np.ix_(S,S)])
       score = inv.sum()
       if score>best_score:
           best_score, best_set = score, S
   ```

   yields $S^*=(1,2,0)$ as well.

---

*Original method published at [https://dx.doi.org/10.2139/ssrn.5118887](https://dx.doi.org/10.2139/ssrn.5118887)*
